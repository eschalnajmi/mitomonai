{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MitoMonai\n",
    "\n",
    "UNet trained on rat and human data. \n",
    "- [Rat Images](https://huggingface.co/datasets/pytc/EM30/resolve/main/EM30-R-im.zip)\n",
    "- [Rat Labels](https://huggingface.co/datasets/pytc/MitoEM/blob/main/EM30-R-mito-train-val-v2.zip)\n",
    "- [Human Images](https://huggingface.co/datasets/pytc/EM30/resolve/main/EM30-H-im.zip)\n",
    "- [Human Labels](https://huggingface.co/datasets/pytc/MitoEM/blob/main/EM30-H-mito-train-val-v2.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    "    LabelToMaskd,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm, Reshape\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "from monai.data import ITKReader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_dir, resource, name: str):\n",
    "\n",
    "    compressed_file = os.path.join(root_dir, name)\n",
    "    \n",
    "    if not os.path.exists(compressed_file):\n",
    "        download_and_extract(resource, compressed_file, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get('MONAI_DATA_DIRECTORY', \"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "get_data(os.path.join(root_dir,\"R\"), \"https://huggingface.co/datasets/pytc/EM30/resolve/main/EM30-R-im.zip\", \"EM30-R-im.zip\")\n",
    "\n",
    "get_data(os.path.join(root_dir,\"R\"), \"https://huggingface.co/datasets/pytc/MitoEM/resolve/main/EM30-R-mito-train-val-v2.zip?download=true\", \"EM30-R-mito-train-val-v2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(os.path.join(os.path.join(root_dir,\"R\"), \"im\", \"im*.png\")))\n",
    "labels = sorted(glob.glob(os.path.join(os.path.join(root_dir,\"R\"), \"mito-train-v2\", \"seg*.tif\")))\n",
    "labels += sorted(glob.glob(os.path.join(os.path.join(root_dir,\"R\"), \"mito-val-v2\", \"seg*.tif\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(images, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files = data_dicts[:400], data_dicts[400:500]\n",
    "\n",
    "print(len(train_files), len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(os.path.join(root_dir,\"H\"), \"https://huggingface.co/datasets/pytc/EM30/resolve/main/EM30-H-im-pad.zip\", \"EM30-H-im-pad.zip\")\n",
    "\n",
    "get_data(os.path.join(root_dir,\"H\"), \"https://huggingface.co/datasets/pytc/MitoEM/resolve/main/EM30-H-mito-train-val-v2.zip?download=true\", \"EM30-H-mito-train-val-v2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(os.path.join(os.path.join(root_dir,\"H\"), \"im_pad\", \"im*.png\")))\n",
    "labels = sorted(glob.glob(os.path.join(os.path.join(root_dir,\"H\"), \"mito-train-v2\", \"seg*.tif\")))\n",
    "labels += sorted(glob.glob(os.path.join(os.path.join(root_dir,\"H\"), \"mito-val-v2\", \"seg*.tif\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(images, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files += data_dicts[:400]\n",
    "val_files += data_dicts[400:500]\n",
    "\n",
    "\n",
    "print(len(train_files), len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(80, 80),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        LabelToMaskd(keys=[\"label\"], select_labels=[0, 1]),  # Ensure labels are within the expected range\n",
    "\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        LabelToMaskd(keys=[\"label\"], select_labels=[0, 1]),  # Ensure labels are within the expected range\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "\n",
    "\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "\n",
    "image = image.unsqueeze(-1).numpy()\n",
    "label = label.unsqueeze(-1).numpy()\n",
    "print(f\"image shape after reshape: {image.shape}, label shape: {label.shape}\")\n",
    "\n",
    "\n",
    "# plot the slice [:, :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:,:,0], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=2)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(sigmoid=True, to_onehot_y=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "save_dir = 'best_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"best_model_{timestamp}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose(AsDiscrete(argmax=True, to_onehot=2))\n",
    "post_label = Compose(AsDiscrete(to_onehot=2))\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print(\"-\" * 10)\n",
    "    #print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        #print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    #print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                roi_size = (160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"best_model_{timestamp}.pth\"))\n",
    "                torch.save(val_data[\"image\"], os.path.join(\"best_val_outputs\", f\"best_val_data_{timestamp}.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    f\"\\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "            #print(\n",
    "             #   f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "              #  f\"\\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "            #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"epoch avg loss\")\n",
    "x = [i+1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x,y,color=\"red\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"val mean dice\")\n",
    "x = [val_interval * (i+1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size=(160,160)\n",
    "        sw_batch_size=4\n",
    "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, model)\n",
    "\n",
    "        plt.figure(\"test\", (18,6))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0,0,:,:], cmap=\"gray\")\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"label\"][0,0,:,:])\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0,:,:])\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.title(f\"masked output {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0,0,:,:], cmap=\"gray\")\n",
    "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0,:,:], alpha=0.5, cmap=\"viridis\")\n",
    "        plt.show()\n",
    "        if i==2:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader=ITKReader),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        LabelToMaskd(keys=[\"label\"], select_labels=[0, 1]),  # Ensure labels are within the expected range\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = Dataset(data=val_files, transform=val_org_transforms)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "model.eval()\n",
    "step = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = (160,160)\n",
    "        sw_batch_size = 4\n",
    "        val_data[\"pred\"] = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "\n",
    "        dice_metric(y_pred=val_outputs.to(device), y=val_labels.to(device))\n",
    "\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "\n",
    "    dice_metric.reset()\n",
    "\n",
    "print(\"metric on og img spacing: \", metric_org)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
